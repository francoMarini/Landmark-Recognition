{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo progetto utilizzeremo il file *train.csv*, scaricabile facilmente dal seguente link:\n",
    " [Google-Landmark](https://www.kaggle.com/google/google-landmarks-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il dataset è composto da 1225029 righe e 3 colonne\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('input/train.csv', delimiter=',')\n",
    "df.dataframeName = 'train.csv'\n",
    "nRow, nCol = df.shape\n",
    "print(f'Il dataset è composto da {nRow} righe e {nCol} colonne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14945\n"
     ]
    }
   ],
   "source": [
    "print(len(df['landmark_id'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il numero complessivo di classi che costituiscono il dataset è **14945**.\n",
    "\n",
    "Osserviamo per ciascun *landmark* quanti sono le corrispondenti immagini associate. \n",
    "\n",
    "Possiamo notare come **70677** immagini non hanno la label associata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None     70677\n",
       "9633     48555\n",
       "6051     47830\n",
       "6599     21780\n",
       "9779     17602\n",
       "         ...  \n",
       "4103         1\n",
       "8472         1\n",
       "14268        1\n",
       "13342        1\n",
       "6069         1\n",
       "Name: landmark_id, Length: 14945, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['landmark_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creazione di un nuovo file .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essendo le classi veramente molto numerose e non potendo contare su elaboratori troppo avanzati, abbiamo deciso di lavorare su un numero limitato di campioni, considerando solo **100 classi**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>landmark_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>70677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9633</td>\n",
       "      <td>48555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6051</td>\n",
       "      <td>47830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6599</td>\n",
       "      <td>21780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9779</td>\n",
       "      <td>17602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14940</th>\n",
       "      <td>4103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14941</th>\n",
       "      <td>8472</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14942</th>\n",
       "      <td>14268</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14943</th>\n",
       "      <td>13342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14944</th>\n",
       "      <td>6069</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14945 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      landmark_id  count\n",
       "0            None  70677\n",
       "1            9633  48555\n",
       "2            6051  47830\n",
       "3            6599  21780\n",
       "4            9779  17602\n",
       "...           ...    ...\n",
       "14940        4103      1\n",
       "14941        8472      1\n",
       "14942       14268      1\n",
       "14943       13342      1\n",
       "14944        6069      1\n",
       "\n",
       "[14945 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.DataFrame(df.landmark_id.value_counts())\n",
    "temp.reset_index(inplace=True)\n",
    "temp.columns = ['landmark_id','count']\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idea è stata quella di considerare solo i landmark caratterizzati da un numero di immagini che si estendesse da 150 a 165."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = temp[(temp[\"count\"] < 165) & (temp[\"count\"] >= 150)]\n",
    "#elimino 1 riga per fare in modo che le classi siano esattamente 100\n",
    "subset = subset.drop(subset.index[len(subset)-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark = subset.iloc[:,:1]\n",
    "l = landmark.values.tolist()\n",
    "m = np.array(l)\n",
    "lista_id = m.reshape(100,)\n",
    "my_dataset = df[df[\"landmark_id\"].isin(lista_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tale file è già presente nella directory *input*, abbiamo voluto dimostrare come è stato ottenuto. \n",
    "\n",
    "Potrebbe essere utile a qualcuno che  volesse considerare un sottoinsieme di classi diverso :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non eseguire, perchè tale file è già presente.\n",
    "my_dataset.to_csv(\"input/train_100classes.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizzazione delle immagini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://lh6.googleusercontent.com/-lLI_nZj5fuk/SMlZL0d1HDI/AAAAAAAACoI/MuwNasDcvdY/s1600/' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://lh6.googleusercontent.com/-u3NHyRs_ZQ0/TZI0Fsh17sI/AAAAAAAABPo/s2ReXmJlWKk/s0-d/' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://lh3.googleusercontent.com/-PDHSz6q_jA8/S8VZfMHVViI/AAAAAAAAFJQ/erPhCzr0HVs/s1600/' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='http://lh3.ggpht.com/-m7cfAZ5SHWE/T1qmsjwKjoI/AAAAAAAAMHw/voDL6yWfO2E/s1600/' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='http://mw2.google.com/mw-panoramio/photos/medium/55327783.jpg' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://lh4.googleusercontent.com/-hs0-UIy-6MA/UdXA_oJkxNI/AAAAAAAAACQ/W_IkVRWtxZc/s150-c/' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='http://lh3.ggpht.com/-aY2rmglPSsw/StjPc-MXq7I/AAAAAAAAD1Y/bK9IMJlMmLk/s1600/' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://lh3.googleusercontent.com/-13A3Bx7tAPM/TAiZqDpe1DI/AAAAAAAALsI/apwlP8rh0hM/s1600/' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://lh4.googleusercontent.com/-n1_5na3IMdQ/Rmp1PVqg0sI/AAAAAAAABD8/NYrkN6OabjQ/s1600/' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='http://lh3.ggpht.com/-p00Kv62DnZ4/TExJplQkZaI/AAAAAAAAAHY/n_-YGy--6UY/s1600/' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://lh5.googleusercontent.com/-4aosBudu3S0/TE89u_3yJKI/AAAAAAAAC-c/mTyhrqLfJJY/s1600/' /><img style='width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;' src='https://lh5.googleusercontent.com/-62o__FBc5KE/UTNsE0NeILI/AAAAAAAABN8/pNp72Kgri7s/s1600/' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "def displayLandmarkImagesLarge(urls, category_name):\n",
    "    img_style = \"width: 200px; height:160px; margin: 0px; float: left; border: 1px solid black;\"\n",
    "    images_list = ''.join([f\"<img style='{img_style}' src='{u}' />\" for _, u in urls.head(12).iteritems()])\n",
    "    display(HTML(images_list))\n",
    "\n",
    "category = df['landmark_id'].value_counts().keys()[15]\n",
    "urls = df[df['landmark_id'] == category]['url']\n",
    "displayLandmarkImagesLarge(urls, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una cosa interessante che si può notare, è il fatto che alcuni *url* relativi alle immagini che andranno a comporre il dataset, **non** sono più disponibili. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download immagini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le immagini verranno scaricate in locale e memorizzate in *sottocartelle*: ciascuna sottocartella conterrà tutte le foto relative ad un particolare *landmark*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.csv', 'train_100classes.csv']\n",
      "Downloading 0/15653\n",
      "Downloading 1/15653\n",
      "Downloading 2/15653\n",
      "Downloading 3/15653\n",
      "Downloading 4/15653\n",
      "Downloading 5/15653\n",
      "Downloading 6/15653\n",
      "Downloading 7/15653\n",
      "Downloading 8/15653\n",
      "Downloading 9/15653\n"
     ]
    }
   ],
   "source": [
    "import sys, requests, shutil, os\n",
    "from urllib import request, error\n",
    "\n",
    "# Input data files are available in the \"input\" directory.\n",
    "print(os.listdir(\"input\"))\n",
    "data=pd.read_csv('input/train_100classes.csv')\n",
    "data.head(5)\n",
    "\n",
    "def fetch_image(path, dir_path):\n",
    "    url=path\n",
    "    response=requests.get(url, stream=True)\n",
    "    with open(dir_path + '/image.jpg', 'wb') as out_file:\n",
    "        shutil.copyfileobj(response.raw, out_file)\n",
    "    del response\n",
    "links=data['url']\n",
    "landmark_id=data['landmark_id']\n",
    "id=data['id']\n",
    "i=0\n",
    "\n",
    "# seperate images into follwing format\n",
    "# folder name = landmark_id, group images with same landmark id into same folder\n",
    "# image name = id.jpg\n",
    "for link in links:              \n",
    "    dir_path = 'input/dataset/' + str(landmark_id[i])\n",
    "    if not os.path.exists(dir_path): \n",
    "    \tos.makedirs(dir_path)\n",
    "    fetch_image(link, dir_path)\n",
    "    os.rename(dir_path + '/image.jpg', dir_path + '/' +  str(id[i]) + '.jpg')\n",
    "    print('Downloading ' + str(i) +  '/' + str(len(links)))\n",
    "    i+=1\n",
    "    #if(i==10):   #uncomment to test in your machine\n",
    "        #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rimuovere foto relative a link non più funzionanti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precedentemente abbiamo visto che non tutti i link che ci vengono forniti risultano essere disponibili. \n",
    "\n",
    "Durante il processo di *download* questi link producono immagini danneggiate, che introducono rumore nel dataset. \n",
    "\n",
    "Per evitare questo problema, bisogna **ripulire il dataset**: le foto relative ai link non funzionanti possiedono una dimensione inferiore ai 5kb, quindi sono molto facilmente riconoscibili e rimuovibili.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input/dataset\\86\\41a8659ae08d069b.jpg\n"
     ]
    }
   ],
   "source": [
    "rootdir = 'input/dataset'\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for f in files:\n",
    "        fullpath = os.path.join(subdir, f)\n",
    "        try:\n",
    "            if os.path.getsize(fullpath) < 5 * 1024:   #set file size in kb\n",
    "                print (fullpath)\n",
    "                os.remove(fullpath)\n",
    "        except WindowsError:\n",
    "            print (\"Error\" + fullpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1355 file* cancellati su 15653"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rimuovere Falsi Positivi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alcune foto, pur avendo una dimensione superiore ai 5kb, non possono essere aperte in maniera corretta e quindi potrebbero causare un errore durante il processo di addestramento della rete.\n",
    "\n",
    "Quindi, queste immagini devono essere eliminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory_path= 'input/dataset/'\n",
    "file1= '86/f0222276f790dd69.jpg'\n",
    "file2= '5140/e59f8417b1b34b09.jpg'\n",
    "file3= '2725/ce3f6f109f9011ad.jpg'\n",
    "file4= '9033/895081fafd0835ec.jpg'\n",
    "file5= '11378/d196fef7b9ff61a7.jpg'\n",
    "file6= '14061/7a27480e313628e9.jpg'\n",
    "file7= '6768/3b5dbec1ad4ffe02.jpg'\n",
    "file8= '7052/e844d973d2f27dd6.jpg'\n",
    "file9= '9763/3e955b5a7ccf07e3.jpg'\n",
    "\n",
    "os.remove(directory_path+file1)\n",
    "os.remove(directory_path+file2)\n",
    "os.remove(directory_path+file3)\n",
    "os.remove(directory_path+file4)\n",
    "os.remove(directory_path+file5)\n",
    "os.remove(directory_path+file6)\n",
    "os.remove(directory_path+file7)\n",
    "os.remove(directory_path+file8)\n",
    "os.remove(directory_path+file9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Falsi Positivi Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo codice permette di stabilire quali sono le immagini del dataset che non possono essere aperte correttamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "folder_path = 'input/dataset'\n",
    "extensions = []\n",
    "for fldr in os.listdir(folder_path):\n",
    "    sub_folder_path = os.path.join(folder_path, fldr)\n",
    "    for filee in os.listdir(sub_folder_path):\n",
    "        file_path = os.path.join(sub_folder_path, filee)\n",
    "        print('** Path: {}  **'.format(file_path), end=\"\\r\", flush=True)\n",
    "        im = Image.open(file_path)\n",
    "        rgb_im = im.convert('RGB')\n",
    "        if filee.split('.')[1] not in extensions:\n",
    "            extensions.append(filee.split('.')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in Train Set, Validation Set e Test Set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso utilizzeremo una particolare *libreria*, che prende il nome di **split-folder** [(info)](https://pypi.org/project/split-folders/).  \n",
    "\n",
    "Questa libreria ci permetterà di *splittare* il dataset iniziale in 3 dataset, che andranno a rappresentare:\n",
    "* il Training Set (**80%**)\n",
    "* il Validation Set (**10%**)\n",
    "* il Test Set (**10%**)\n",
    "\n",
    "mantenendo l'organizzazione iniziale, cioè una cartella per ciascun landmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 9 files [00:00, 143.24 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "\n",
    "input='input/dataset'\n",
    "output='input/data'\n",
    "splitfolders.ratio(input, output=output, seed=1337, ratio=(.8, .1, .1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
